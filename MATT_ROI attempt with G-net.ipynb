{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4da158-fa4a-43be-9750-7c28261c8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Written by Matthew Miller, adapted from Diana Lewis and chatgpt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6958e2-663c-42e6-b090-dd281701f8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Input Images from File to train the model \n",
    "# Specify the folder containing the images and CSV file\n",
    "folder_path = 'Glaucoma_Balanced_Dataset/Preprocessed_images_input_to_bound'\n",
    "\n",
    "# Load preprocessed images\n",
    "images = []\n",
    "for i in range(30):  # Assuming 30 images starting from 000000\n",
    "    image_filename = f'enhanced_TRAIN{i:06d}.jpg'  # Zero-padded 6-digit number\n",
    "    image_path = os.path.join(folder_path, image_filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    images.append(image)\n",
    "    \n",
    "# Load bounding box labels from CSV file\n",
    "bounding_box_labels = []\n",
    "image_filenames = []  # List to store image filenames\n",
    "csv_file = os.path.join(folder_path, 'Glaucoma30_bounding_boxes_06Apr2024.csv')\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip header row\n",
    "    for row in csvreader:\n",
    "        # Extract image filename from column 4 (assuming 0-based indexing)\n",
    "        image_filename = row[3].strip()  # Remove leading/trailing whitespaces\n",
    "        image_filenames.append(image_filename)  # Add filename to the list\n",
    "        # Extract bounding box parameters from the \"rect\" column\n",
    "        bbox_params = eval(row[5])  # Assuming \"rect\" is the 6th column\n",
    "        # Extract individual bounding box parameters from the list of dictionaries\n",
    "        bbox_dict = bbox_params[0]  # Assuming there's only one dictionary in the list\n",
    "        x = int(bbox_dict['x'])\n",
    "        y = int(bbox_dict['y'])\n",
    "        width = int(bbox_dict['width'])\n",
    "        height = int(bbox_dict['height'])\n",
    "\n",
    "        ##### Adjust bounding box parameters (trial and error)\n",
    "        #x_adjusted = x * 21  # Adjust x-coordinate by adding 10 pixels\n",
    "        #y_adjusted = y * 10 # Adjust y-coordinate by adding 10 pixels\n",
    "        #width_adjusted = width * 21  # Adjust width by scaling by a factor of 1.1\n",
    "        #height_adjusted = height * 10  # Adjust height by scaling by a factor of 1.1\n",
    "        ##### Add bounding box parameters to the list\n",
    "        #bounding_box_labels.append([x, y, width, height])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Matt found the real way to deal the image image size mismatch\n",
    "        #label studio just gives the percentages of the overall image size for the bounding box\n",
    "        #Here's the non trial and error method:\n",
    "        \n",
    "        #These are needed to show what image size the boxes have to go back to\n",
    "        original_width = 2048\n",
    "        original_height = 1024\n",
    "        \n",
    "        \n",
    "        x_adjusted = int(x / 100.0 * original_width)\n",
    "        y_adjusted = int(y / 100.0 * original_height)\n",
    "        width_adjusted = int(width / 100.0 * original_width)\n",
    "        height_adjusted = int(height / 100.0 * original_height)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        bounding_box_labels.append([x_adjusted, y_adjusted, width_adjusted, height_adjusted])\n",
    "\n",
    "        # Process bounding box parameters as needed\n",
    "        # Example: Print adjusted bounding box parameters\n",
    "        '''print(f\"Image: {image_filename}, Adjusted Bounding Box: x={x_adjusted}, y={y_adjusted}, \"\n",
    "                  f\"width={width_adjusted}, height={height_adjusted}\")\n",
    "                    '''\n",
    "# Convert bounding box labels to numpy array\n",
    "labels = np.array(bounding_box_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adbfcf27",
   "metadata": {},
   "source": [
    "# Print lengths of images and bounding_box_labels lists\n",
    "print(\"Number of images:\", len(images))\n",
    "print(\"Number of bounding box labels:\", len(bounding_box_labels))\n",
    "\n",
    "# Print contents of bounding_box_labels list\n",
    "#for idx, label in enumerate(bounding_box_labels):\n",
    "    #print(f\"Label {idx + 1}:\", label)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of images in X_train:\", len(X_train))\n",
    "print(\"Number of labels in y_train:\", len(y_train))\n",
    "\n",
    "# Print a few samples from X_train and y_train\n",
    "for i in range(min(30, len(X_train))):\n",
    "    print(\"Image shape:\", X_train[i].shape)\n",
    "    print(\"Label:\", y_train[i])\n",
    "\n",
    "#DEBUGGING\n",
    "#print(\"Image filenames:\", image_filenames)\n",
    "#print(\"Image filename from CSV:\", image_filename)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "X_val_np = np.array(X_val)\n",
    "y_val_np = np.array(y_val)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train_np.shape)\n",
    "print(\"Shape of y_train:\", y_train_np.shape)\n",
    "\n",
    "# Check shapes\n",
    "#print(\"Shape of X_train:\", X_train.shape)\n",
    "#print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_val:\", X_val_np.shape)\n",
    "print(\"Shape of y_val:\", y_val_np.shape)\n",
    "\n",
    "# Inspect a few samples\n",
    "print(\"Sample image from X_train:\", X_train[0].shape)\n",
    "print(\"Sample bounding box from y_train:\", y_train[0])\n",
    "print(\"Sample image from X_val:\", X_val[0].shape)\n",
    "print(\"Sample bounding box from y_val:\", y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81004bce-32dc-49e0-905c-9506f5672802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image filename from CSV: enhanced_TRAIN000000.jpg\n",
      "Index of image filename: [0]\n",
      "Bounding box label: [245, 235, 327, 368]\n",
      "Image filename from CSV: enhanced_TRAIN000001.jpg\n",
      "Index of image filename: [1]\n",
      "Bounding box label: [1433, 194, 327, 337]\n",
      "Image filename from CSV: enhanced_TRAIN000002.jpg\n",
      "Index of image filename: [2]\n",
      "Bounding box label: [512, 440, 225, 235]\n",
      "Image filename from CSV: enhanced_TRAIN000003.jpg\n",
      "Index of image filename: [3]\n",
      "Bounding box label: [430, 235, 307, 358]\n",
      "Image filename from CSV: enhanced_TRAIN000004.jpg\n",
      "Index of image filename: [4]\n",
      "Bounding box label: [1187, 337, 245, 256]\n",
      "Image filename from CSV: enhanced_TRAIN000005.jpg\n",
      "Index of image filename: [5]\n",
      "Bounding box label: [1249, 378, 266, 286]\n",
      "Image filename from CSV: enhanced_TRAIN000006.jpg\n",
      "Index of image filename: [6]\n",
      "Bounding box label: [1228, 153, 450, 450]\n",
      "Image filename from CSV: enhanced_TRAIN000007.jpg\n",
      "Index of image filename: [7]\n",
      "Bounding box label: [1331, 184, 348, 368]\n",
      "Image filename from CSV: enhanced_TRAIN000008.jpg\n",
      "Index of image filename: [8]\n",
      "Bounding box label: [1228, 409, 204, 204]\n",
      "Image filename from CSV: enhanced_TRAIN000009.jpg\n",
      "Index of image filename: [9]\n",
      "Bounding box label: [204, 296, 307, 327]\n",
      "Image filename from CSV: enhanced_TRAIN000010.jpg\n",
      "Index of image filename: [10]\n",
      "Bounding box label: [573, 368, 204, 215]\n",
      "Image filename from CSV: enhanced_TRAIN000011.jpg\n",
      "Index of image filename: [11]\n",
      "Bounding box label: [1146, 378, 245, 256]\n",
      "Image filename from CSV: enhanced_TRAIN000012.jpg\n",
      "Index of image filename: [12]\n",
      "Bounding box label: [143, 174, 286, 317]\n",
      "Image filename from CSV: enhanced_TRAIN000013.jpg\n",
      "Index of image filename: [13]\n",
      "Bounding box label: [593, 430, 204, 245]\n",
      "Image filename from CSV: enhanced_TRAIN000014.jpg\n",
      "Index of image filename: [14]\n",
      "Bounding box label: [593, 286, 286, 307]\n",
      "Image filename from CSV: enhanced_TRAIN000015.jpg\n",
      "Index of image filename: [15]\n",
      "Bounding box label: [1167, 327, 184, 215]\n",
      "Image filename from CSV: enhanced_TRAIN000016.jpg\n",
      "Index of image filename: [16]\n",
      "Bounding box label: [1351, 296, 368, 399]\n",
      "Image filename from CSV: enhanced_TRAIN000017.jpg\n",
      "Index of image filename: [17]\n",
      "Bounding box label: [1269, 327, 245, 245]\n",
      "Image filename from CSV: enhanced_TRAIN000018.jpg\n",
      "Index of image filename: [18]\n",
      "Bounding box label: [1269, 317, 204, 225]\n",
      "Image filename from CSV: enhanced_TRAIN000019.jpg\n",
      "Index of image filename: [19]\n",
      "Bounding box label: [430, 327, 327, 358]\n",
      "Image filename from CSV: enhanced_TRAIN000020.jpg\n",
      "Index of image filename: [20]\n",
      "Bounding box label: [389, 174, 327, 348]\n",
      "Image filename from CSV: enhanced_TRAIN000021.jpg\n",
      "Index of image filename: [21]\n",
      "Bounding box label: [593, 430, 225, 235]\n",
      "Image filename from CSV: enhanced_TRAIN000022.jpg\n",
      "Index of image filename: [22]\n",
      "Bounding box label: [430, 317, 389, 430]\n",
      "Image filename from CSV: enhanced_TRAIN000023.jpg\n",
      "Index of image filename: [23]\n",
      "Bounding box label: [368, 184, 348, 368]\n",
      "Image filename from CSV: enhanced_TRAIN000024.jpg\n",
      "Index of image filename: [24]\n",
      "Bounding box label: [1208, 358, 245, 245]\n",
      "Image filename from CSV: enhanced_TRAIN000025.jpg\n",
      "Index of image filename: [25]\n",
      "Bounding box label: [225, 317, 266, 307]\n",
      "Image filename from CSV: enhanced_TRAIN000026.jpg\n",
      "Index of image filename: [26]\n",
      "Bounding box label: [1413, 337, 286, 286]\n",
      "Image filename from CSV: enhanced_TRAIN000027.jpg\n",
      "Index of image filename: [27]\n",
      "Bounding box label: [1208, 368, 245, 235]\n",
      "Image filename from CSV: enhanced_TRAIN000028.jpg\n",
      "Index of image filename: [28]\n",
      "Bounding box label: [593, 368, 225, 245]\n",
      "Image filename from CSV: enhanced_TRAIN000029.jpg\n",
      "Index of image filename: [29]\n",
      "Bounding box label: [1372, 235, 348, 348]\n"
     ]
    }
   ],
   "source": [
    "# Load bounding box labels from CSV file\n",
    "csv_file = os.path.join(folder_path, 'Glaucoma30_bounding_boxes_06Apr2024.csv')\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip header row\n",
    "    for row in csvreader:\n",
    "        # Extract image filename from the 4th column\n",
    "        image_filename = row[3]  # Assuming image filename is in the 4th column\n",
    "        print(\"Image filename from CSV:\", image_filename)\n",
    "        \n",
    "        # Find the index of the image filename in the list of image filenames\n",
    "        image_index = [i for i, filename in enumerate(image_filenames) if filename == image_filename]\n",
    "        print(\"Index of image filename:\", image_index)\n",
    "        \n",
    "        # Print the bounding box label corresponding to the image\n",
    "        bounding_box_label = bounding_box_labels[image_index[0]]\n",
    "        print(\"Bounding box label:\", bounding_box_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c1be1c-5462-44cb-9e7f-aecdcddb8122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(len(images)):\\n    image = images[i].copy()  # Make a copy of the image to avoid modifying the original\\n    bbox = labels[i]  # Get the bounding box for the current image\\n    # Draw bounding box on image\\n    image_with_bbox = draw_bounding_box(image, bbox)\\n    # Display image with bounding box\\n    plt.imshow(image_with_bbox)\\n    plt.title(f\"Image {i+1} with Bounding Box\")\\n    plt.axis(\\'off\\')\\n    plt.show()\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to draw bounding boxes on images\n",
    "def draw_bounding_box(image, bbox):\n",
    "    x_adjusted, y_adjusted, width_adjusted, height_adjusted = bbox\n",
    "    # Draw bounding box rectangle on image\n",
    "    #cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.rectangle(image, (x_adjusted, y_adjusted), (x_adjusted + width_adjusted, y_adjusted + height_adjusted), (0, 255, 0), 4)\n",
    "    return image\n",
    "\n",
    "# Visualize images with bounding boxes\n",
    "'''for i in range(len(images)):\n",
    "    image = images[i].copy()  # Make a copy of the image to avoid modifying the original\n",
    "    bbox = labels[i]  # Get the bounding box for the current image\n",
    "    # Draw bounding box on image\n",
    "    image_with_bbox = draw_bounding_box(image, bbox)\n",
    "    # Display image with bounding box\n",
    "    plt.imshow(image_with_bbox)\n",
    "    plt.title(f\"Image {i+1} with Bounding Box\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "## Matt: FYI these images are showing as blueish instead of redish since you have to display it as BRG instead of RGB.\n",
    "## Look at some of my other code if you're interested in seeing how you display it correctly\n",
    "## Leaving it as blue has no impact on the later steps, it's just a matter of how it displays\n",
    "\n",
    "\n",
    "##use half the width to make a radius and make a circular mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e023118-86cd-4ab5-ba0e-565f15c9b4a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor image_mask in masks_train:\\n    \\n    plt.imshow(image_mask, cmap='gray')  # Display the mask as a grayscale image\\n    plt.title('Mask Image')\\n    plt.colorbar()  # Add color bar to show pixel intensity values\\n    plt.show()\\n    \\n    \\n    \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_binary_mask(image_shape, bbox):\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "    x, y, width, height = bbox\n",
    "    mask[y:y+height, x:x+width] = 1\n",
    "    return mask\n",
    "\n",
    "def convert_bboxes_to_masks(images, bboxes):\n",
    "    masks = []\n",
    "    for image, bbox in zip(images, bboxes):\n",
    "        mask = create_binary_mask(image.shape[:2], bbox)\n",
    "        masks.append(mask)\n",
    "    return masks\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train contains input images and y_train contains bounding box labels\n",
    "masks_train = convert_bboxes_to_masks(images, labels)\n",
    "#masks_val = convert_bboxes_to_masks(X_val, y_val)\n",
    "\n",
    "'''\n",
    "for image_mask in masks_train:\n",
    "    \n",
    "    plt.imshow(image_mask, cmap='gray')  # Display the mask as a grayscale image\n",
    "    plt.title('Mask Image')\n",
    "    plt.colorbar()  # Add color bar to show pixel intensity values\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fded96fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(images))\n",
    "print(len(masks_train))\n",
    "type(masks_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f751c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64966924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc79cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 61s/step - accuracy: 0.5469 - loss: 32.5103 - val_accuracy: 0.9355 - val_loss: 1.5339 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55s/step - accuracy: 0.6746 - loss: 3.8415 "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def gnet_cup(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(34, (4, 2), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(34, (4, 2), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(4, 4))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(4, 4))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(4, 4))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(4, 4))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    up6 = UpSampling2D(size=(4, 4))(conv5)\n",
    "    merge6 = concatenate([conv4, up6], axis=3)\n",
    "    conv6 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = UpSampling2D(size=(4, 4))(conv6)\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = UpSampling2D(size=(4, 4))(conv7)\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(64, 4, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = UpSampling2D(size=(4, 4))(conv8)\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(34, (4, 2), activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(34, (4, 2), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(num_classes, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (2048, 1024, 3)\n",
    "num_classes = 1\n",
    "cup_model = gnet_cup(input_shape, num_classes)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, masks_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "data_gen_args = dict(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1,\n",
    "                     zoom_range=0.1, horizontal_flip=True, vertical_flip=True, fill_mode='reflect')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 5\n",
    "\n",
    "# Define seed for reproducibility\n",
    "seed = 1\n",
    "\n",
    "# Reshape input data\n",
    "X_train = np.reshape(X_train, (-1, input_shape[0], input_shape[1], input_shape[2]))\n",
    "X_val = np.reshape(X_val, (-1, input_shape[0], input_shape[1], input_shape[2]))\n",
    "y_train = np.reshape(y_train, (-1, input_shape[0], input_shape[1], 1)).astype(np.float32)\n",
    "y_val = np.reshape(y_val, (-1, input_shape[0], input_shape[1], 1)).astype(np.float32)\n",
    "\n",
    "# Create custom generators for training and validation data\n",
    "def custom_generator(image_data_generator, mask_data_generator, X, y, batch_size, seed):\n",
    "    image_generator = image_data_generator.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_data_generator.flow(y, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    # Get an iterator for the generators\n",
    "    image_iterator = iter(image_generator)\n",
    "    mask_iterator = iter(mask_generator)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            X_batch = next(image_iterator)\n",
    "            y_batch = next(mask_iterator)\n",
    "            yield X_batch, y_batch\n",
    "        except StopIteration:\n",
    "            # Reset iterators if they reach the end of the data\n",
    "            image_iterator = iter(image_generator)\n",
    "            mask_iterator = iter(mask_generator)\n",
    "\n",
    "\n",
    "train_generator = custom_generator(image_datagen, mask_datagen, X_train, y_train, batch_size, seed)\n",
    "val_generator = custom_generator(image_datagen, mask_datagen, X_val, y_val, batch_size, seed)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "opt = Adam(clipvalue=0.5)\n",
    "cup_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = cup_model.fit(train_generator, steps_per_epoch=len(X_train) // batch_size,\n",
    "                        validation_data=val_generator, validation_steps=len(X_val) // batch_size,\n",
    "                        epochs=5, callbacks=[reduce_lr])\n",
    "# Save the trained model\n",
    "cup_model.save(\"ROI_model.h5\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd261d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ddc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc03325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07742222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91659f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca3263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19e744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8f908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53313e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply to new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model(\"ROI_model.h5\")\n",
    "\n",
    "# Assuming new_images is a numpy array of new input images\n",
    "# Normalize the new images\n",
    "new_images_normalized = new_images / 255.0\n",
    "\n",
    "# Make predictions (generate masks) using the loaded model\n",
    "predicted_masks = loaded_model.predict(new_images_normalized)\n",
    "\n",
    "# Display the first predicted mask (assuming predicted_masks[0] is the first mask)\n",
    "plt.imshow(predicted_masks[0][:, :, 0], cmap='gray')  # Display the first mask (assuming grayscale)\n",
    "plt.title('Predicted Mask')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52efcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca3232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1faed105-6033-499d-9cbe-5fa075c48b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024\n'y' sizes: 24\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 57\u001b[0m\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#model.summary()\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val))\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     60\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:114\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    110\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024\n'y' sizes: 24\n"
     ]
    }
   ],
   "source": [
    "#Using U-Net Architecture to construct FCN for Segmentation of Optic Disk and Optic Cup \n",
    "#Performed on 30 pre-processed images which have bouding boxes around the area of interest and are cropped to uniform size \n",
    "#Use these 30 images to train the FCN to continue this segmentation with remaining images \n",
    "\n",
    "def unet (input_shape): \n",
    "    inputs = tf.keras.Input (shape = input_shape)\n",
    "\n",
    "    #Encoder\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    # Decoder\n",
    "    up5 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4)\n",
    "    up5 = layers.concatenate([up5, conv3], axis=3)\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(up5)\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5)\n",
    "    up6 = layers.concatenate([up6, conv2], axis=3)\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv6)\n",
    "    up7 = layers.concatenate([up7, conv1], axis=3)\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Set input shape and create model\n",
    "input_shape = (1024, 2048 , 3)  # Adjust according to your input image size\n",
    "model = unet(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "#model.summary()\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=8, epochs=50, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "# Optionally, save the trained model\n",
    "# Example: model.save('fcn_model.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a0837-21ef-45e4-9b3b-e077398940d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa0def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25981c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d4f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
