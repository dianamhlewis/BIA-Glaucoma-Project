{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3121ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code written by Matthew Miller (adapted from chatGPT and prior code from class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9266cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7a3e7",
   "metadata": {},
   "source": [
    "## Set the image folder and read the CSV file (the CSV files and the images must be in a single folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89636848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glaucoma Dataset/train_0_first_50\\JustRAIGS_Train_labels_first50.csv\n"
     ]
    }
   ],
   "source": [
    "# Folder containing the image files and the corresponding CSV file\n",
    "folder_path = \"Glaucoma Dataset/train_0_first_50\"\n",
    "csv_file = \"JustRAIGS_Train_labels_first50.csv\"\n",
    "csv_path = os.path.join(folder_path,csv_file)\n",
    "print(csv_path)\n",
    "\n",
    "# Read the CSV file containing the image filenames and classifications\n",
    "data = pd.read_csv(csv_path)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860a8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty lists to store the image data and corresponding classifications\n",
    "images = []\n",
    "classifications = []\n",
    "\n",
    "# Define the target size for resizing the images\n",
    "## This step is limiting our resolution, probably way too aggressive eventually, but fine for testing\n",
    "target_size = (100, 100)  # Adjust the size as needed\n",
    "\n",
    "#target_size = (1944, 1944)  # this should make the images square initally and size down to the most common\n",
    "#smallest dimension for a few of the images.\n",
    "#Going larger may force interpolation, which wouldn't be ideal.\n",
    "# this method is also distorting the aspect ratio, but that won't matter later since the optic disk is what we care about,\n",
    "#and we can define a constant size for that after preprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each row in the CSV file\n",
    "for index, row in data.iterrows():\n",
    "    # Read the image file\n",
    "    image_filename = row['Eye ID']  # The column containing the image filenames\n",
    "    #print(image_filename)\n",
    "    image_path = os.path.join(folder_path, image_filename)\n",
    "    #print(image_path)\n",
    "    image = cv2.imread(image_path)  # Use cv2.imread for reading images\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Preprocess the image as necessary (e.g., resizing, hist eq, normalization)  \n",
    "    # Preprocess the image by resizing it to the target size\n",
    "    image_resized = cv2.resize(image, target_size)\n",
    "    \n",
    "    \n",
    "    b, g, r = cv2.split(image_resized)\n",
    "    # Perform histogram equalization and normalization on each channel\n",
    "    b_eq = cv2.equalizeHist(b)\n",
    "    g_eq = cv2.equalizeHist(g)\n",
    "    r_eq = cv2.equalizeHist(r)\n",
    "    # Merge the equalized channels back into a color image\n",
    "    image_eq = cv2.merge((b_eq, g_eq, r_eq))\n",
    "    \n",
    "    # Normalize the image intensity values\n",
    "    image_eq = cv2.normalize(image_eq, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    \n",
    "   \n",
    "    # Add the preprocessed image to the images list\n",
    "    images.append(image_eq)\n",
    "    \n",
    "    # Display the image if testing is necessary(RGB stuff is so the images display with right colors (BGR to RGB))\n",
    "    #image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "    #plt.imshow(image_rgb)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Get the classification label\n",
    "    classification = row['Final Label']  # Assuming 'Classification' is the column containing the classifications\n",
    "    classifications.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f80bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to numpy arrays for further processing\n",
    "images = np.array(images)\n",
    "#print(images)\n",
    "\n",
    "#This next line works correctly for reading the classifications\n",
    "classifications = np.array(classifications)\n",
    "#print(classifications)\n",
    "# Now you can use the images and classifications for training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fe6bb",
   "metadata": {},
   "source": [
    "## Need to split the data now into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0058fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 40\n",
      "Number of validation samples: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "#0.2 is setting the validation data size to 20% of the total (80/20 split) and the random state is the seed\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, classifications, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the training and validation sets\n",
    "print(\"Number of training samples:\", len(X_train))\n",
    "print(\"Number of validation samples:\", len(X_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bece81",
   "metadata": {},
   "source": [
    "## This is our neural network, the parameters and network architecture are just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdcf2be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 268ms/step - accuracy: 0.6583 - loss: 29.0725 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Validation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Build the Neural Network Model\n",
    "#change the image height and width (100x100 for this example to fit the output of the preprocessed images above)\n",
    "image_height = 100\n",
    "image_width = 100\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "#history = model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=1, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "#test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "#print('Test accuracy:', test_acc)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Validation accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
